{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Two Sigma Financial Modelling Challenge](https://www.kaggle.com/c/two-sigma-financial-modeling)\n",
    "Can you uncover predictive value in an uncertain world? Hosted by [Kaggle](https://www.kaggle.com)\n",
    "\n",
    "#### In this notebook\n",
    "1. Preprocess\n",
    "2. Model (LSTM)\n",
    "3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Here are all the modules we will be using in this notebook. Make sure you can import everything before moving on!'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "\n",
    "data_root = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the training data into training, validation and test sets holding pairwise disjoint timestamps.\n",
    "These dataframes may share instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_hdf(data_root + 'train.h5')\n",
    "\n",
    "num_time_stamps = len(raw_df['timestamp'].unique())\n",
    "valid_start_ind = int(num_time_stamps * 0.5)\n",
    "test_start_ind = int(num_time_stamps * 0.8)\n",
    "\n",
    "raw_train_df = raw_df[raw_df['timestamp'] < valid_start_ind]\n",
    "# raw_valid_df = raw_df[(raw_df['timestamp'] < test_start_ind) & (raw_df['timestamp'] >= valid_start_ind)]\n",
    "# raw_test_df = raw_df[(raw_df['timestamp'] <= test_start_ind)]\n",
    "\n",
    "raw_df = None # Release memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we create a Transformer\n",
    "\n",
    "__Fit:__ Calculates the median, mean and std of each column\n",
    "\n",
    "__Transform:__\n",
    "\n",
    "* Fills the missing values with the median of each column\n",
    "  * Here I chose median to prevent the outliers from shifting the values by a large margin\n",
    "* Limits the values within each column to 5 std from the average to remove the outliers\n",
    "* Normalizes the features (subtract the mean and divide by std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, untouched=['id', 'timestamp', 'y']):\n",
    "        '''untouched is a list of column names to ignore when transforming the dataframes'''\n",
    "        self.untouched = untouched\n",
    "    def fit(self, df):\n",
    "        '''df is a dataframe to extract metadata (median, mean and std, of each column) '''\n",
    "        self.median = df.median()\n",
    "        self.mean = df.mean()\n",
    "        self.std = df.std()\n",
    "    def transform(self, df):\n",
    "        '''\n",
    "        Given a dataframe,\n",
    "        it fills the missing attributes with the median of each column,\n",
    "        limits the values of each column to between 5 STD from the mean,\n",
    "        and normalizes the features\n",
    "        '''\n",
    "        # Fill the missing attributes with the median of each column\n",
    "        df.fillna(self.median, inplace=True)\n",
    "        # Limit the values of each column to between 5 STD from the mean\n",
    "        col_names = [col for col in df.columns if col not in self.untouched]\n",
    "        for cn in col_names:\n",
    "            lower = self.mean[cn] - 5 * self.std[cn]\n",
    "            upper = self.mean[cn] + 5 * self.std[cn]\n",
    "            df.loc[:, cn] = df.loc[:, cn].clip(lower=lower, upper=upper)\n",
    "        # Normalize the features\n",
    "        for cn in col_names:\n",
    "            df.loc[:, cn] = (df.loc[:, cn] - self.mean[cn]) / self.std[cn]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = Transformer()\n",
    "T.fit(raw_train_df)\n",
    "\n",
    "train_df = T.transform(raw_train_df)\n",
    "raw_train_df = None # Release Memory\n",
    "print('processed training dataframe')\n",
    "\n",
    "# valid_df = T.transform(raw_valid_df)\n",
    "# raw_valid_df = None # Release Memory\n",
    "# print('processed validation dataframe')\n",
    "\n",
    "# test_df = T.transform(raw_test_df)\n",
    "# raw_test_df = None # Release Memory\n",
    "# print('processed test dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, num_feat, lstm_size, fc_hidd_size, lr):\n",
    "        self.num_feat = num_feat\n",
    "        self.lstm_size = lstm_size\n",
    "        self.fc_hidd_size = fc_hidd_size\n",
    "        self.lr = lr\n",
    "        self.build_graph()\n",
    "    def build_graph(self):\n",
    "        '''Input'''\n",
    "        self.X = tf.placeholder(dtype=tf.float64, shape=[None, None, self.num_feat], name='X')\n",
    "        self.y = tf.placeholder(dtype=tf.float64, shape=[None, None, 1], name='y')\n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int64, shape=[None], name='sequence_length')\n",
    "        self.target_mask = tf.placeholder(dtype=tf.float64, shape=[None, None, 1], name='target_mask')\n",
    "        \n",
    "        '''LSTM'''\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self.lstm_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        # The initial state of the LSTM\n",
    "        state_shape = [None, self.lstm_size]\n",
    "        self.state_c = tf.placeholder(dtype=tf.float64, shape=state_shape)\n",
    "        self.state_h = tf.placeholder(dtype=tf.float64, shape=state_shape)\n",
    "        initial_state = tf.nn.rnn_cell.LSTMStateTuple(self.state_c, self.state_h)\n",
    "        \n",
    "        self.output, self.state = tf.nn.dynamic_rnn(\n",
    "            cell=lstm_cell,\n",
    "            dtype=tf.float64,\n",
    "            sequence_length=self.sequence_length,\n",
    "            initial_state=initial_state,\n",
    "            inputs=self.X\n",
    "        )\n",
    "        \n",
    "        self.f0 = tf.contrib.layers.fully_connected(\n",
    "            self.output, num_outputs=self.fc_hidd_size,\n",
    "            activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.pred = tf.contrib.layers.fully_connected(\n",
    "            self.f0, num_outputs=1, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.pred_masked = tf.mul(self.pred, self.target_mask)\n",
    "        \n",
    "        '''Loss'''\n",
    "        self.errors = tf.squared_difference(self.y, self.pred_masked)\n",
    "        self.loss = tf.reduce_mean(self.errors)\n",
    "        \n",
    "        '''Optimize'''\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.lr,\n",
    "            beta1=0.9, beta2=0.999,\n",
    "            epsilon=1e-08,\n",
    "            use_locking=False,\n",
    "            name='Adam'\n",
    "        ).minimize(self.loss)\n",
    "        \n",
    "    def train_on_batch(self, session, sequence, sequence_targets, target_mask, sequence_length):\n",
    "        batch_size = len(sequence)\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.X: sequence,\n",
    "            self.y: sequence_targets,\n",
    "            self.target_mask: target_mask,\n",
    "            self.sequence_length: sequence_length,\n",
    "            self.state_c: np.zeros(shape=[batch_size, self.lstm_size]),\n",
    "            self.state_h: np.zeros(shape=[batch_size, self.lstm_size])\n",
    "        }\n",
    "\n",
    "        loss = session.run([self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, session, sequence, sequence_length, initial_state=None):\n",
    "        feed_dict = {self.X: sequence, self.sequence_length: sequence_length}\n",
    "        if initial_state is None:\n",
    "            batch_size = len(sequence)\n",
    "            feed_dict[self.state_c] = np.zeros(shape=[batch_size, self.lstm_size])\n",
    "            feed_dict[self.state_h] = np.zeros(shape=[batch_size, self.lstm_size])\n",
    "        else:\n",
    "            feed_dict[self.state_c] = initial_state.c\n",
    "            feed_dict[self.state_h] = initial_state.h\n",
    "        \n",
    "        pred, state = session.run([self.pred, self.state], feed_dict=feed_dict)\n",
    "        return pred, state        \n",
    "        \n",
    "    def fit(self, session, input_df, num_epoch, batch_size):\n",
    "        examples = {}\n",
    "        for _id, df in input_df.groupby('id'):\n",
    "            exp = []\n",
    "            exp.append(df.drop(['id', 'y'], axis=1).values)\n",
    "            exp.append(df['y'].values)\n",
    "            exp.append(df.shape[0])\n",
    "            examples[_id] = exp\n",
    "                \n",
    "        keys = examples.keys()\n",
    "        num_seq = len(keys)\n",
    "        num_batch = ((num_seq / batch_size) + 1) * num_epoch\n",
    "        \n",
    "        for _ in range(num_batch):\n",
    "            batch_keys = np.random.choice(keys, batch_size, False)\n",
    "            batch = [examples[k] for k in batch_keys]\n",
    "            X, y, ln = zip(*batch)\n",
    "            \n",
    "            max_len = max(ln)\n",
    "            feat = [np.pad(s, ((0, max_len - s.shape[0]), (0, 0)), 'constant') for s in X]\n",
    "            tar = [np.pad(t, ((0, max_len - t.shape[0])), 'constant') for t in y]\n",
    "            mask = [np.pad(np.ones_like(t), ((0, max_len - t.shape[0])), 'constant') for t in y]\n",
    "            feat, tar, mask = np.array(feat), np.expand_dims(np.array(tar), 2), np.expand_dims(np.array(mask), 2)\n",
    "            \n",
    "            loss = self.train_on_batch(session, feat, tar, mask, ln)\n",
    "            yield loss[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__dyn_plot__ helps visualize the learning process dynamically. For details, read the answer by [HYRY](http://stackoverflow.com/users/772649/hyry):\n",
    "\n",
    "* [how to dynamically update a plot in a loop in ipython notebook within one cell](http://stackoverflow.com/questions/21360361/how-to-dynamically-update-a-plot-in-a-loop-in-ipython-notebook-within-one-cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dyn_plot(history):\n",
    "    plt.clf()\n",
    "    plt.plot(history)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "num_epoch = 400\n",
    "batch_size = 128\n",
    "num_feat = 109\n",
    "lstm_size = 1024\n",
    "fc_hidd_size = 256\n",
    "lr = 1e-3\n",
    "\n",
    "M = Model(num_feat=num_feat, lstm_size=lstm_size, fc_hidd_size=fc_hidd_size, lr=lr)\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "    \n",
    "history = []\n",
    "for loss in M.fit(session, train_df, num_epoch, batch_size):\n",
    "    history.append(loss)\n",
    "    dyn_plot(np.sqrt(history))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
